# -*- coding: utf-8 -*-
"""Copy of Final pproject.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lyxfjZKNMt56YKcMW9zLDT43bnP7jXNO
"""

!pip install xgboost lightgbm catboost

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from urllib.parse import urlparse
import re

from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import (
    classification_report, confusion_matrix,
    roc_auc_score, accuracy_score, f1_score, roc_curve
)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import SVC
from sklearn.neighbors import KNeighborsClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.neural_network import MLPClassifier

!pip install -q scikit-learn pandas matplotlib seaborn
import pandas as pd
import numpy as np
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay, roc_auc_score, roc_curve
import matplotlib.pyplot as plt
import seaborn as sns
from google.colab import files

# Upload CSV file
uploaded = files.upload()

# Replace the filename below if different
file_name = list(uploaded.keys())[0]

# Load data and skip bad lines
df = pd.read_csv(file_name, on_bad_lines='skip')

# Check basic info
print("Dataset shape:", df.shape)
print(df.head())

# Separate features and labels
X = df.drop(columns=['URL', 'Label'], errors='ignore')  # drops URL and Label if they exist
y = df['Label']

# Standardize features
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)

# Train Random Forest model
model = RandomForestClassifier(n_estimators=100, random_state=42)
model.fit(X_train, y_train)

# Predictions
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

# Evaluation
print("\nClassification Report:")
print(classification_report(y_test, y_pred))

# Confusion Matrix
cm = confusion_matrix(y_test, y_pred)
disp = ConfusionMatrixDisplay(confusion_matrix=cm)
disp.plot()
plt.title("Confusion Matrix")
plt.show()

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, y_proba)
roc_auc = roc_auc_score(y_test, y_proba)

plt.figure()
plt.plot(fpr, tpr, label=f'ROC Curve (area = {roc_auc:.2f})')
plt.plot([0, 1], [0, 1], linestyle='--')
plt.xlabel("False Positive Rate")
plt.ylabel("True Positive Rate")
plt.title("ROC Curve")
plt.legend(loc="lower right")
plt.grid(True)
plt.show()

!pip install -q catboost imbalanced-learn
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, accuracy_score, f1_score, roc_auc_score
from sklearn.preprocessing import StandardScaler
from imblearn.over_sampling import SMOTE
from catboost import CatBoostClassifier
from urllib.parse import urlparse
import re

# Upload dataset
from google.colab import files
uploaded = files.upload()
file_path = list(uploaded.keys())[0]

#Clean malformed lines (optional but safer)
with open(file_path, 'r', encoding='utf-8') as f:
    lines = f.readlines()

expected_cols = len(lines[0].strip().split(','))
clean_lines = [line for line in lines if len(line.strip().split(',')) == expected_cols]

clean_file = "/content/phishing_dataset_with_fyzxeatures (1).csv"
with open(clean_file, 'w', encoding='utf-8') as f:
    f.writelines(clean_lines)

#Load dataset
df = pd.read_csv(clean_file)

#Feature engineering function
def extract_url_features(url):
    features = {}
    features['url_length'] = len(url)
    features['num_digits'] = sum(c.isdigit() for c in url)
    features['num_special_chars'] = len(re.findall(r'[^\w]', url))
    features['has_https'] = int(url.startswith('https'))
    features['count_dot'] = url.count('.')
    features['count_at'] = url.count('@')
    features['count_dash'] = url.count('-')
    features['count_double_slash'] = url.count('//')
    features['has_ip'] = int(bool(re.search(r'[0-9]+\.[0-9]+\.[0-9]+\.[0-9]+', url)))
    features['has_login'] = int('login' in url.lower())
    features['has_verify'] = int('verify' in url.lower())
    features['has_update'] = int('update' in url.lower())
    features['has_account'] = int('account' in url.lower())
    features['has_secure'] = int('secure' in url.lower())
    features['domain_length'] = len(urlparse(url).netloc)
    return features

#Apply feature extraction
df_features = df['URL'].apply(extract_url_features).apply(pd.Series)
df_final = pd.concat([df_features, df['Label']], axis=1)

#Features and labels
X = df_final.drop('Label', axis=1)
y = df_final['Label']

#Balance classes using SMOTE
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

sm = SMOTE(random_state=42)
X_resampled, y_resampled = sm.fit_resample(X_scaled, y)

#Train-test split
X_train, X_test, y_train, y_test = train_test_split(X_resampled, y_resampled, test_size=0.2, random_state=42)

#Train CatBoost model
model = CatBoostClassifier(verbose=0, random_state=42)
model.fit(X_train, y_train)

#Evaluate model
y_pred = model.predict(X_test)
y_proba = model.predict_proba(X_test)[:, 1]

print("Classification Report:")
print(classification_report(y_test, y_pred))
print(f"Accuracy : {accuracy_score(y_test, y_pred):.4f}")
print(f"F1 Score : {f1_score(y_test, y_pred):.4f}")
print(f"ROC AUC  : {roc_auc_score(y_test, y_proba):.4f}")

# Import all necessary libraries first
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.svm import LinearSVC
from sklearn.neighbors import KNeighborsClassifier
from sklearn.neural_network import MLPClassifier
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier

from sklearn.svm import LinearSVC

# Updated model list
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "Linear SVM": LinearSVC(),  # Faster SVM alternative
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(),
    "CatBoost": CatBoostClassifier(verbose=0),
    "AdaBoost": AdaBoostClassifier(),
    "Neural Network (MLP)": MLPClassifier(max_iter=500)
}

results = []

for name, model in models.items():
    print(f"\n Training {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)

    # For models with probability support
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)[:, 1]
        roc_auc = roc_auc_score(y_test, y_prob)
    else:
        y_prob = None
        roc_auc = "N/A"

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)

    print(classification_report(y_test, y_pred))
    print(f" Accuracy: {acc:.4f}")
    print(f" F1 Score: {f1:.4f}")
    print(f" ROC AUC: {roc_auc}")

    results.append({
        "Model": name,
        "Accuracy": acc,
        "F1 Score": f1,
        "ROC AUC": roc_auc
    })

# Convert results to DataFrame
results_df = pd.DataFrame(results).sort_values(by="F1 Score", ascending=False)
results_df.reset_index(drop=True, inplace=True)
results_df

import matplotlib.pyplot as plt

# Commented out IPython magic to ensure Python compatibility.
# %matplotlib inline

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, auc

plt.figure(figsize=(10, 8))

for name, model in models.items():
    if hasattr(model, "predict_proba"):
        y_prob = model.predict_proba(X_test)[:, 1]
        fpr, tpr, _ = roc_curve(y_test, y_prob)
        roc_auc = auc(fpr, tpr)
        plt.plot(fpr, tpr, label=f"{name} (AUC = {roc_auc:.2f})")

plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve Comparison')
plt.legend()
plt.grid()
plt.show()

# Step 1: Check columns
print(df.columns)
!pip install tldextract --quiet


# Step 2: Rename if necessary
if 'URL' in df.columns:
    df.rename(columns={'URL': 'url'}, inplace=True)

# Step 3: Run feature engineering
import tldextract
import re
from urllib.parse import urlparse

def calculate_entropy(string):
    from collections import Counter
    import math
    prob = [freq / len(string) for freq in Counter(string).values()]
    return -sum(p * math.log2(p) for p in prob)

df['URL_Len'] = df['url'].apply(len)
df['Digits'] = df['url'].apply(lambda x: sum(c.isdigit() for c in x))
df['Special_Chars'] = df['url'].apply(lambda x: len(re.findall(r'\W', x)))
df['Subdomains'] = df['url'].apply(lambda x: len(tldextract.extract(x).subdomain.split('.')) if tldextract.extract(x).subdomain else 0)
df['Entropy'] = df['url'].apply(calculate_entropy)

import pandas as pd
from sklearn.model_selection import train_test_split

# Load the already-engineered dataset
df = pd.read_csv('/content/phishing_dataset_with_fyzxeatures (1).csv')  # or the correct filename you saved

# Map column names if needed
df.rename(columns={'Label': 'label'}, inplace=True)

# Use the final feature set
features = ['URL Len', 'Digits', 'Special Chars', 'Entropy', 'Subdomains']
X = df[features]
y = df['label']

# Split for training
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier
from sklearn.neighbors import KNeighborsClassifier
# from sklearn.svm import SVC  #  Removed SVM
from xgboost import XGBClassifier
from lightgbm import LGBMClassifier
from catboost import CatBoostClassifier
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import accuracy_score, f1_score, classification_report, roc_auc_score

#Updated model list without SVM
models = {
    "Logistic Regression": LogisticRegression(),
    "Decision Tree": DecisionTreeClassifier(),
    "Random Forest": RandomForestClassifier(),
    "K-Nearest Neighbors": KNeighborsClassifier(),
    "XGBoost": XGBClassifier(use_label_encoder=False, eval_metric='logloss'),
    "LightGBM": LGBMClassifier(),
    "CatBoost": CatBoostClassifier(verbose=0),
    "AdaBoost": AdaBoostClassifier(),
    "Neural Network (MLP)": MLPClassifier(max_iter=500)
}

results = []

for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else "N/A"

    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc}")

    results.append({
        "Model": name,
        "Accuracy": acc,
        "F1 Score": f1,
        "ROC AUC": roc_auc
    })

import joblib
from sklearn.pipeline import Pipeline

# Select the best model based on your results
catboost_model = models["CatBoost"]  # Change to your best model: e.g., models["XGBoost"]

# Define your selected features (as used in training)
selected_features = ['URL Len', 'Digits', 'Special Chars', 'Subdomains', 'Entropy']

# Create a pipeline (you can add pre-processing like scaling if needed)
model_pipeline = Pipeline([
    ('classifier', catboost_model)
])

# Fit the pipeline on the full dataset
model_pipeline.fit(X[selected_features], y)

# Save the pipeline model
joblib.dump(model_pipeline, 'phishing_detector_model.pkl')

print("Model saved as 'phishing_detector_model.pkl'")

# Required imports
import pandas as pd
import numpy as np
import joblib
import tldextract
import re
from urllib.parse import urlparse
from sklearn.model_selection import train_test_split
from sklearn.pipeline import Pipeline
from catboost import CatBoostClassifier

#  Load your dataset
df = pd.read_csv('/content/phishing_dataset_with_fyzxeatures (1).csv')  # Change if filename differs

#  Rename column if needed
if 'URL' in df.columns:
    df.rename(columns={'URL': 'url'}, inplace=True)
if 'Label' in df.columns:
    df.rename(columns={'Label': 'label'}, inplace=True)

# Feature engineering functions
def count_digits(url):
    return sum(char.isdigit() for char in url)

def count_special_chars(url):
    return len(re.findall(r'\W', url))

def calculate_entropy(url):
    prob = [float(url.count(c)) / len(url) for c in dict.fromkeys(list(url))]
    return -sum([p * np.log2(p) for p in prob])

def get_subdomain_count(url):
    ext = tldextract.extract(url)
    return ext.subdomain.count('.') + 1 if ext.subdomain else 0

def get_url_length(url):
    return len(url)

#  Apply features
df['URL Len'] = df['url'].apply(get_url_length)
df['Digits'] = df['url'].apply(count_digits)
df['Special Chars'] = df['url'].apply(count_special_chars)
df['Entropy'] = df['url'].apply(calculate_entropy)
df['Subdomains'] = df['url'].apply(get_subdomain_count)

# Prepare features and labels
selected_features = ['URL Len', 'Digits', 'Special Chars', 'Entropy', 'Subdomains']
X = df[selected_features]
y = df['label']

# Train CatBoost on full dataset
catboost_model = CatBoostClassifier(verbose=0)
catboost_model.fit(X, y)

# Build pipeline (for future inference)
model_pipeline = Pipeline([
    ('classifier', catboost_model)
])

#  Save model
joblib.dump(model_pipeline, 'phishing_detector_model.pkl')
print(" Model saved as 'phishing_detector_model.pkl'")

# Store trained models in a dictionary
trained_models = {}

for name, model in models.items():
    print(f"\nTraining {name}...")
    model.fit(X_train, y_train)

    # Save each trained model to a variable
    trained_models[name] = model  # Save for later visualization

    y_pred = model.predict(X_test)
    y_prob = model.predict_proba(X_test)[:, 1] if hasattr(model, "predict_proba") else None

    acc = accuracy_score(y_test, y_pred)
    f1 = f1_score(y_test, y_pred)
    roc_auc = roc_auc_score(y_test, y_prob) if y_prob is not None else "N/A"

    print(classification_report(y_test, y_pred))
    print(f"Accuracy: {acc:.4f}")
    print(f"F1 Score: {f1:.4f}")
    print(f"ROC AUC: {roc_auc}")

    results.append({
        "Model": name,
        "Accuracy": acc,
        "F1 Score": f1,
        "ROC AUC": roc_auc
    })

!pip install flask flask-ngrok pyngrok joblib

!ngrok config add-authtoken 30W3SLThEJhOfJE0HZW8vDPWkmm_4rizoefrAbX4L5ctq1Dva

from google.colab import files
uploaded = files.upload()

from flask import Flask, request
from pyngrok import ngrok
import pandas as pd
import joblib
import sqlite3
import time
from datetime import datetime

# Load model
model = joblib.load("phishing_detector_model.pkl")

# Feature extractor
def extract_features(url):
    return {
        "URL Len": len(url),
        "Digits": sum(c.isdigit() for c in url),
        "Special Chars": sum(not c.isalnum() for c in url),
        "Entropy": len(set(url)) / len(url) if len(url) > 0 else 0,
        "Subdomains": url.count('.') - 1
    }

# DB setup
conn = sqlite3.connect('phishing_log.db', check_same_thread=False)
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        url TEXT,
        result TEXT,
        probability REAL,
        time_taken REAL,
        timestamp TEXT
    )
''')
conn.commit()

app = Flask(__name__)

@app.route('/', methods=['GET', 'POST'])
def index():
    result = ""
    if request.method == 'POST':
        url = request.form['url']
        try:
            start = time.time()

            features = extract_features(url)
            df = pd.DataFrame([features])
            prediction = model.predict(df)[0]
            prob = model.predict_proba(df)[0][1]

            # Result logic
            label = "Phishing" if prediction == 1 else "Legitimate"
            color = "red" if prediction == 1 else "green"
            bar_width = int(prob * 100)

            result = f"""
            <div class='prediction' style='color:{color}; font-weight:bold;'>{'⚠️' if prediction == 1 else '✅'} {label} URL</div>
            <div class='prob'>Probability: {prob:.2%}</div>
            <div style="margin-top: 10px; width: 100%; background: #eee; border-radius: 10px; height: 20px;">
                <div style="width: {bar_width}%; height: 100%; background: {color}; border-radius: 10px;"></div>
            </div>
            """

            # Save to DB
            process_time = round(time.time() - start, 4)
            timestamp = datetime.now().strftime("%Y-%m-%d %H:%M:%S")
            cursor.execute("INSERT INTO logs (url, result, probability, time_taken, timestamp) VALUES (?, ?, ?, ?, ?)",
                           (url, label, prob, process_time, timestamp))
            conn.commit()

        except Exception as e:
            result = f"<span class='phishing'>❌ Error: {str(e)}</span>"

    return f"""
    <!DOCTYPE html>
    <html>
    <head>
        <meta charset="UTF-8">
        <title>Phishing Detector</title>
        <style>
            body {{
                font-family: 'Segoe UI', sans-serif;
                background: linear-gradient(to right, #141e30, #243b55);
                color: white;
                display: flex;
                flex-direction: column;
                align-items: center;
                justify-content: center;
                height: 100vh;
            }}
            .card {{
                background: white;
                color: black;
                padding: 40px;
                border-radius: 16px;
                box-shadow: 0 10px 30px rgba(0, 0, 0, 0.2);
                text-align: center;
                width: 400px;
                max-width: 90%;
            }}
            input[type="text"], input[type="submit"] {{
                font-size: 16px;
                padding: 12px;
                margin-top: 10px;
                width: 100%;
                border-radius: 8px;
                border: 1px solid #ccc;
            }}
            input[type="submit"] {{
                background-color: #0066ff;
                color: white;
                cursor: pointer;
            }}
            input[type="submit"]:hover {{
                background-color: #004bb5;
            }}
            .result {{
                margin-top: 20px;
                font-size: 18px;
                font-weight: bold;
            }}
        </style>
    </head>
    <body>
        <div class="card">
            <h1>🔐 Phishing URL Detector</h1>
            <form method="post">
                <input name="url" type="text" placeholder="Enter a URL to check..." required />
                <input type="submit" value="🔎 Check URL" />
            </form>
            <div class="result">{result}</div>
        </div>
    </body>
    </html>
    """

# Optional: View logs (accessible at /logs)
@app.route('/logs')
def logs():
    cursor.execute("SELECT * FROM logs ORDER BY id DESC LIMIT 20")
    rows = cursor.fetchall()
    html = "<h2 style='color:white;text-align:center;'>Recent Detection Logs</h2><table border=1 style='color:white;margin:auto;'>"
    html += "<tr><th>ID</th><th>URL</th><th>Result</th><th>Probability</th><th>Time (s)</th><th>Timestamp</th></tr>"
    for r in rows:
        html += f"<tr><td>{r[0]}</td><td>{r[1]}</td><td>{r[2]}</td><td>{r[3]:.2%}</td><td>{r[4]}</td><td>{r[5]}</td></tr>"
    html += "</table>"
    return html

# Ngrok tunnel
public_url = ngrok.connect(5000)
print("🚀 Your app is live at:", public_url)
app.run(port=5000)

import sqlite3
import pandas as pd

conn = sqlite3.connect("phishing_log.db")
df_logs = pd.read_sql_query("SELECT * FROM logs ORDER BY id DESC", conn)

conn.close()
df_logs.head()

import sqlite3

conn = sqlite3.connect("phishing_log.db")
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        url TEXT,
        result TEXT,
        probability REAL,
        time_taken REAL,
        timestamp DATETIME DEFAULT CURRENT_TIMESTAMP
    )
''')
conn.commit()
conn.close()
print("✅ Table 'logs' is now created.")

!pip install ipywidgets

import sqlite3

# Initialize the database and create table if not exists
conn = sqlite3.connect("phishing_log.db")
cursor = conn.cursor()
cursor.execute('''
    CREATE TABLE IF NOT EXISTS logs (
        id INTEGER PRIMARY KEY AUTOINCREMENT,
        url TEXT NOT NULL,
        result TEXT NOT NULL,
        probability REAL,
        timestamp TEXT
    )
''')
conn.commit()
conn.close()

import datetime

conn = sqlite3.connect("phishing_log.db")
cursor = conn.cursor()
cursor.execute('''
    INSERT INTO logs (url, result, probability, timestamp)
    VALUES (?, ?, ?, ?)
''', (url, "Phishing" if prediction == 1 else "Legitimate", float(prob), datetime.datetime.now().isoformat()))
conn.commit()
conn.close()